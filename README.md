# Introduction to WebCrawler
Collaborative work with teammate to implement graph with created websites environment. 


# Projects Goals: 
+ Practice  using third-party libraries
+ Apply knowledge of graphs to real-world problem
+ Apply understandings of time complexity'
+ Enhance problem-solving skills

## Problems: 


## Description


## Requirements


## User Manual


## Reflection


## Results



## Function explantions:
validator library: used to fastly validate url such as email, websites, and
Beautiful Soup library: Beautiful Soup is a Python package for parsing HTML and XML documents, including those with malformed markup. It creates a parse tree for documents that can be used to extract data from HTML, which is useful for web scraping.


## Tasks:
- Workflow tasks:
+ Validate URLs and depth function(Riley)
+ Retrieve function HTML from each URLs (Danny)
+ Parse url from href on the current webpage function (Danny)
+ Create a graph in memory and update function for correct depth (Riley is king)
+ Print graph function
+ closeness computation (Danny)
+ Print closeness computation process : '1+2 = 3(Danny)'
- Program Output Tasks:
+ Parse HTML function (Danny)
+ Find function for link (href attribute reference) (Danny)
+ Encrypting function for URL_list files
+ Graphical display - Graph representation (both)
+ Generate function to a CSV file of crawled webpage's closeness. as output
+ Function measuring centrality.